[
  {
    "objectID": "science.html",
    "href": "science.html",
    "title": "Georgie Samaha",
    "section": "",
    "text": "Bioinformatics and high performance computing\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanine health and population genomics\n\n\n\n\n\n\n\n\n\n\n\n\n\nCross-species applications of genomic tools\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating optimised genomics workflows\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeline models of complex human disease\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlattening the Nextflow learning curve\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenomic resources for rare peripheral neuropathies\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking bioinformatics workflows accessible\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision diagnostics in cancer\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "science/byod.html",
    "href": "science/byod.html",
    "title": "Making bioinformatics workflows accessible",
    "section": "",
    "text": "Working at the command line is challenging for life scientists, many of whom have received no formal computer science training. The Australian BioCommons Bring Your Own Data platforms project is focused on providing the life science community with curated workflows, tools, training and support across Australian command line infrastructures.\n\nMy role\n\nWorkflows theme lead\nDeveloped public, scalable, reproducible data processing workflows\nConceptualised national training events focused on the accessibility of bioinformatics workflows at the command-line\nDelivered and facilitated workshops and webinars as a part of the Australian BioCommons training programme\nLiased with and coordinated activites involving partners at various computational infrastructures, service providers, research communities across the country\nAdvocated for the needs and interests of the life science community to national computational infrastructures and service providers\n\n\n\nOutcomes\n\nInstallation and optimisation of bioinformatics workflows across command line infrastructures at NCI, Pawsey, QRIScloud, and the University of Melbourne Research Computing Services\nEnabled 60 high-impact publications\nSupported 11 research groups\nDeveloped 19 public workflows\n1,600 genomes processed at national HPCs\n1,000 transcriptomes processed at national HPCs\nUpskilled 1,000 life scientists\n\n\n\nSkills\n\nData analysis, software engineering, project management, verbal and written communication\nDisciplines: bioinformatics\nPlatforms: HPC, cloud, RStudio\nTools: bash, R, python, PBS, SLURM, git, GitHub, Nextflow, Singularity, Docker, sHPC"
  },
  {
    "objectID": "science/nf-template.html",
    "href": "science/nf-template.html",
    "title": "Flattening the Nextflow learning curve",
    "section": "",
    "text": "Workflow frameworks, like Nextflow, have recently emerged as solutions to the challenges posed by bioinformatics pipelines. Bioinformatics data processing typically comprises many individual steps that accept and output varying file formats, and require the use of multiple coding languages and pieces of software. While Nextflow offers a nice solution for creating reproducible, portable workflows that are easy to run, it comes at the cost of a steep learning curve. Nextflow’s users need to learn yet another Domain-Specific Language (DSL) and a new and foreign programming paradigm in order to get the most out of Nextflow.\nAs a part of the Australian BioCommons Bring Your Own Data platforms project, we prioritised Nextflow over other workflow frameworks in our workflow development activities, developer support, and training programme due to its growing popularity in the community.\n\nMy role\n\nDeveloped a Nextflow workflow template that provided a framework and recommendations to workflow developers new to Nextflow\nDeveloped and deployed Nextflow workflows on HPC and cloud platforms\nDeployed nf-core workflows on HPC and cloud platforms\nConfigured and optimised Nextflow and nf-core workflows for national compute platforms and Nextflow Tower\nDeveloped and delivered Nextflow and nf-core training events to a national audience\n\nContributed to and liased with the Nextflow and nf-core communities\n\n\n\nOutcomes\n\nNextflow DSL2 template GitHub repository\nCustomising nf-core workshop\nRNAseq workshop\nProject currently in progress\n\n\n\nSkills\n\nData analysis, software engineering, project management, verbal and written communication\nDisciplines: bioinformatics\nPlatforms: HPC, cloud, RStudio\nTools: bash, R, python, PBS, SLURM, git, GitHub, Nextflow, Singularity, Docker"
  },
  {
    "objectID": "science/webinars.html",
    "href": "science/webinars.html",
    "title": "Bioinformatics and high performance computing",
    "section": "",
    "text": "Bioinformaticians and life scientists are increasingly turning to high performance computing infrastructure and efficient, scalable workflows as their research becomes more data intensive. Accessing and getting the most out of these facilities can be challenging for many of us who have never received formal computational sciences training. This webinar series introduces the high-performance computing facilities and services available to Australian researchers and ways to access and use them to do high impact research.\n\nMy role\n\nConceptualise and deliver webinars to a national audience\n\n\n\nOutcomes\n\nWebinar: Where to go when your bioinformatics runs out of compute\nWebinar: High performance bioinformatics: submitting your best NCMAS application\nWebinar: Getting started with mapping and variant calling on the command line\nWebinar: Australian HPCs for bioinformatics\n\n\n\nSkills\n\nVerbal and written communication\nDisciplines: bioinformatics, high performance computing"
  },
  {
    "objectID": "science/structuralV.html",
    "href": "science/structuralV.html",
    "title": "Genomic resources for rare peripheral neuropathies",
    "section": "",
    "text": "Like many other rare and heritable conditions, mapping the causative genetic loci of peripheral neuropathies can be challenging due to a lack of available population-level data and standardised methodologies available to the research community. I have worked closely and collaboratively with the Australian motor neuropathies community to develop reproducible and sensitive protocols for identifying clinically relevant genomic events in patient cohorts and families.\n\nMy role\n\nDesign and implement a protocol for filtering variants under expected modes of inheritance\nDesign and implement a public, sensitive, and reproducible workflow for identifying and annotating structural variation\nIdentify candidate causative genomic events\nLiase and collaborate with bioinformaticians, clinicians, and geneticists to optimise a bioinformatics workflow\n\n\n\nOutcomes\n\nWorkflow: GermlineStructuralV-nf\nResults currently under embargo\n\n\n\nSkills\n\nSoftware engineering, data analysis, project management, verbal and written communication\nDisciplines: bioinformatics, genomics, genetics\n\nPlatforms: HPC, cloud\nTools: bash, Nextflow, Nextflow Tower, python, PBS, SLURM, Singularity, git, GitHub"
  },
  {
    "objectID": "science/110dogs.html",
    "href": "science/110dogs.html",
    "title": "Canine health and population genomics",
    "section": "",
    "text": "This project involved the processing of a large whole genome sequence dataset comprising 113 samples. To process these samples, we developed and applied two highly scalable workflows to realign all samples to a more recent version of the canine reference genome assembly and call short variants (SNVs and indels) across the genome. The scale of the dataset required the use of the National Computational Infrastructure’s Gadi high performance computer.\n\nMy role\n\nDeveloped and integrated processes for non-human samples into an existing public workflow\nRealigned and called short variants at NCI Gadi\nGenerated a large-scale multi-breed database of short variants\nOnboarded and supported researcher in accessing and using NCI Gadi\n\n\n\nOutcomes\n\nPublication: GWAS for Chronic Superficial Keratitis in the Australian Racing Greyhound\nPublication: De-novo and genome-wide meta-analyses identify a risk haplotype for congenital sensorineural deafness in Dalmatian dogs\n\n\n\nSkills\n\nData analysis, project management, verbal and written communication\nDisciplines: genomics, bioinformatics\nPlatforms: HPC\nTools: bash, perl, markdown, git, GitHub, PBS"
  },
  {
    "objectID": "science/wildcats.html",
    "href": "science/wildcats.html",
    "title": "Cross-species applications of genomic tools",
    "section": "",
    "text": "While genomics has enabled vast improvements in the quantification of genome-wide diversity and the identification of adaptive and deleterious alleles in model species, wildlife species have not reaped the same benefits. In resource constrained species, alternative genomic approaches that reduce costs and computational resources can be used to inform conservation management. In the absence of species-specific reference genomes, the availability of a reference genome from a closely related species can provide a reliable substrate for performing variant discovery. Domestic cats benefit from high-quality reference genome assemblies and share a high degree of genomic synteny with their wild counterparts. In this project, I evaluated and demonstrated the value of cross-species application of various genomic resources developed for the domesitc cat in the Sumatran tiger, snow leopard and cheetah.\n\nMy role\n\nConceptualised all aspects of the research for this project\nLiased with veterinarians and zoo managers to collect samples\nExtracted genomic DNA from whole blood and buccal swabs\nProcessed and analysed whole genome sequencing data\nPerformed in silico simulation of genotyping array and reduced representation sequencing datasets\nPerformed and published a literature review and original manuscript\n\n\n\nOutcomes\n\nPublication: Exploiting genomic synteny in Felidae: cross-species genome alignments and SNV discovery can aid conservation management\nOther results currently under embargo\n\n\n\nSkills\n\nWet lab, data analysis, project management, verbal and written communication\nDisciplines: genetics, genomics, conservation genetics, bioinformatics\nPlatforms: HPC, RStudio, Excel\n\nTools: bash, R, python, PBS, git, GitHub"
  },
  {
    "objectID": "science/burmese.html",
    "href": "science/burmese.html",
    "title": "Feline models of complex human disease",
    "section": "",
    "text": "Cat breeds can serve as informative natural genetic models of human disease because of their similarity to humans in terms of genetics, anatomy, physiology, and our shared environment. By studying cat breeds with a high prevalence of a hertiable disease, like feline diabetes mellitus (FDM) in Burmese cats, we can gain insights into the underlying genetic mechanisms of the diseases.\nFor this project, we explored the genetic landscape of FDM in a population of Australian and European Burmese cats. We used multiple techniques and technologies to identify regions of disease association and selection in affected cats.\n\nMy role\n\nConceptualised all aspects of the research for this project\nLiased with veterinarians and breeders to collect samples\nExtracted genomic DNA from whole blood and buccal swabs\nProcessed and analysed Illumina genotyping array and whole genome sequencing data\nIdentified candidate genes and variants for FDM\nPerformed and published a literature review and original manuscript\n\n\n\nOutcomes\n\nWorkflow: Probe Remap workflow\nPublication: The Burmese cat as a genetic model of type 2 diabetes in humans\nPublication: Mapping the genetic basis of diabetes mellitus in the Australian Burmese cat (Felis catus)\n\n\n\nSkills\n\nWet lab, data analysis, project management, verbal and written communication\nDisciplines: genetics, genomics, bioinformatics\nPlatforms: HPC, RStudio, Excel\n\nTools: bash, R, python, PBS, GitHub"
  },
  {
    "objectID": "science/cancer.html",
    "href": "science/cancer.html",
    "title": "Precision diagnostics in cancer",
    "section": "",
    "text": "Whole-genome profiling of somatic mutations allows researchers to develop new insights into the underlying causes of cancers and develop new treatments and prevention strategies. However, genetic profiling of poorly understood cancers can be challenging due to the complexity and heterogeneity of somatic mutations across patient populations, and lacking population-level data. Best practice bioinformatics workflows that provide a standardised approach to somatic mutation identification and annotation can address these challenges and provide clinicians with viable treatment targets.\n\nMy role\n\nIdentified and implemented best practice bioinformatic tools and public datasets\nImplemented 2 best pratice workflows to provide patient reports and identify short and structural variants in a cohort of thyroid cancer patients\nIdentified candidate genes for poorly differentiated thyroid cancer\n\n\n\nOutcomes\n\nWorkflow under development\nResults under embargo\n\n\n\nSkills\n\nData analysis, project management, verbal and written communication\nDisciplines: cancer genetics, genomics, bioinformatics\nPlatforms: HPC, cloud, RStudio\n\nTools: bash, R, python, Nextflow, PBS, git, GitHub"
  },
  {
    "objectID": "science/parabricks.html",
    "href": "science/parabricks.html",
    "title": "Evaluating optimised genomics workflows",
    "section": "",
    "text": "As a part of the Australian BioCommons Bring Your Own Data platforms project, we work closely with partners from the National Computational Infrastructure (NCI), who were looking to make scalable bioinformatics data processing more accessible at their facilities. We worked with NCI to evaluate the performance of NVIDIA Parabricks software suite at their Gadi HPC. GPU-accelerated tools, like Parabricks, are currently rare in bioinformatics but have the capacity to significantly reduce the time and computational resources required for complex data processing like mapping and variant calling.\n\nMy role\n\nDesign and implement a technical performance evaluation protocol\nDesign and implement a biological validation protocol\nRun and compare the performance of multiple optimised genomic workflows at NCI Gadi\nReport findings and recommendations to NCI, NVIDIA, and Australian BioCommons regarding the potential impact of the availability of Parabricks at NCI\n\n\n\nOutcomes\n\nWorkflow: GermlineShortV Biovalidation\nReport: Evaluation of Mapping and Germline Variant Calling Pipelines on Australian High-Performance Computing Facilities\nAccess to Parabricks licenses at NCI Gadi at no cost to researchers\n\n\n\nSkills\n\nData analysis, project management, verbal and written communication\nDisciplines: bioinformatics, genomics\nPlatforms: HPC\n\nTools: bash, PBS, git, GitHub"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Georgie Samaha",
    "section": "",
    "text": "I’m a community-focused bioinformatician, working on making bioinformatics more approachable for all life scientists. I’m the Bioinformatics Group Lead at the Sydney Informatics Hub, University of Sydney, working with Australian BioCommons."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Georgie Samaha",
    "section": "",
    "text": "I work at the Sydney Informatics Hub, University of Sydney, and Australian BioCommons, directly supporting Australian researchers by developing open-source tools and solutions and consulting on reproducible research practices and best-practice bioinformatics.\nI am the product owner for the Australian BioCommons’ BioCLI and University of Sydney’s Sydney Research Cloud projects. My interest lies in making computing infrastructure and bioinformatics methods more approachable, particularly for life scientists who are new to compute-based research."
  },
  {
    "objectID": "about.html#key-research-interests",
    "href": "about.html#key-research-interests",
    "title": "Georgie Samaha",
    "section": "Key research interests",
    "text": "Key research interests"
  },
  {
    "objectID": "about.html#professional-affiliations",
    "href": "about.html#professional-affiliations",
    "title": "Georgie Samaha",
    "section": "Professional affiliations",
    "text": "Professional affiliations"
  },
  {
    "objectID": "projects/byod.html",
    "href": "projects/byod.html",
    "title": "Making bioinformatics workflows accessible",
    "section": "",
    "text": "Working at the command line is challenging for life scientists, many of whom have received no formal computer science training. The Australian BioCommons Bring Your Own Data platforms project is focused on providing the life science community with curated workflows, tools, training and support across Australian command line infrastructures.\n\nMy role\n\nWorkflows theme lead\nDeveloped public, scalable, reproducible data processing workflows\nConceptualised national training events focused on the accessibility of bioinformatics workflows at the command-line\nDelivered and facilitated workshops and webinars as a part of the Australian BioCommons training programme\nLiased with and coordinated activites involving partners at various computational infrastructures, service providers, research communities across the country\nAdvocated for the needs and interests of the life science community to national computational infrastructures and service providers\n\n\n\nOutcomes\n\nInstallation and optimisation of bioinformatics workflows across command line infrastructures at NCI, Pawsey, QRIScloud, and the University of Melbourne Research Computing Services\nEnabled 60 high-impact publications\nSupported 11 research groups\nDeveloped 19 public workflows\n1,600 genomes processed at national HPCs\n1,000 transcriptomes processed at national HPCs\nUpskilled 1,000 life scientists\n\n\n\nSkills\n\nData analysis, software engineering, project management, verbal and written communication\nDisciplines: bioinformatics\nPlatforms: HPC, cloud, RStudio\nTools: bash, R, python, PBS, SLURM, git, GitHub, Nextflow, Singularity, Docker, sHPC"
  },
  {
    "objectID": "projects/nf-template.html",
    "href": "projects/nf-template.html",
    "title": "Flattening the Nextflow learning curve",
    "section": "",
    "text": "Workflow frameworks, like Nextflow, have recently emerged as solutions to the challenges posed by bioinformatics pipelines. Bioinformatics data processing typically comprises many individual steps that accept and output varying file formats, and require the use of multiple coding languages and pieces of software. While Nextflow offers a nice solution for creating reproducible, portable workflows that are easy to run, it comes at the cost of a steep learning curve. Nextflow’s users need to learn yet another Domain-Specific Language (DSL) and a new and foreign programming paradigm in order to get the most out of Nextflow.\nAs a part of the Australian BioCommons Bring Your Own Data platforms project, we prioritised Nextflow over other workflow frameworks in our workflow development activities, developer support, and training programme due to its growing popularity in the community.\n\nMy role\n\nDeveloped a Nextflow workflow template that provided a framework and recommendations to workflow developers new to Nextflow\nDeveloped and deployed Nextflow workflows on HPC and cloud platforms\nDeployed nf-core workflows on HPC and cloud platforms\nConfigured and optimised Nextflow and nf-core workflows for national compute platforms and Nextflow Tower\nDeveloped and delivered Nextflow and nf-core training events to a national audience\n\nContributed to and liased with the Nextflow and nf-core communities\n\n\n\nOutcomes\n\nNextflow DSL2 template GitHub repository\nCustomising nf-core workshop\nRNAseq workshop\nSeqera platform for Australian researchers\n\n\n\nSkills\n\nData analysis, software engineering, project management, verbal and written communication\nDisciplines: bioinformatics\nPlatforms: HPC, cloud, RStudio\nTools: bash, R, python, PBS, SLURM, git, GitHub, Nextflow, Singularity, Docker"
  },
  {
    "objectID": "projects/webinars.html",
    "href": "projects/webinars.html",
    "title": "Bioinformatics and high performance computing training",
    "section": "",
    "text": "Bioinformaticians and life scientists are increasingly turning to high performance computing infrastructure and efficient, scalable workflows as their research becomes more data intensive. Accessing and getting the most out of these facilities can be challenging for many of us who have never received formal computational sciences training. This webinar series introduces the high-performance computing facilities and services available to Australian researchers and ways to access and use them to do high impact research.\n\nMy role\n\nConceptualise and deliver webinars to a national audience\n\n\n\nOutcomes\n\nWebinar: Pro tips for scaling bioinformatics workflows for HPC\nWebinar: Where to go when your bioinformatics runs out of compute\nWebinar: High performance bioinformatics: submitting your best NCMAS application\nWebinar: Getting started with mapping and variant calling on the command line\n\n\n\nSkills\n\nVerbal and written communication\nDisciplines: bioinformatics, high performance computing"
  },
  {
    "objectID": "projects/structuralV.html",
    "href": "projects/structuralV.html",
    "title": "Genomic resources for rare peripheral neuropathies",
    "section": "",
    "text": "Like many other rare and heritable conditions, mapping the causative genetic loci of peripheral neuropathies can be challenging due to a lack of available population-level data and standardised methodologies available to the research community. I have worked closely and collaboratively with the Australian motor neuropathies community to develop reproducible and sensitive protocols for identifying clinically relevant genomic events in patient cohorts and families.\n\nMy role\n\nDesign and implement a protocol for filtering variants under expected modes of inheritance\nDesign and implement a public, sensitive, and reproducible workflow for identifying and annotating structural variation\nIdentify candidate causative genomic events\nLiase and collaborate with bioinformaticians, clinicians, and geneticists to optimise a bioinformatics workflow\n\n\n\nOutcomes\n\nWorkflow: GermlineStructuralV-nf\nResults currently under embargo"
  },
  {
    "objectID": "projects/110dogs.html",
    "href": "projects/110dogs.html",
    "title": "Canine health and population genomics",
    "section": "",
    "text": "This project involved the processing of a large whole genome sequence dataset comprising 113 samples. To process these samples, we developed and applied two highly scalable workflows to realign all samples to a more recent version of the canine reference genome assembly and call short variants (SNVs and indels) across the genome. The scale of the dataset required the use of the National Computational Infrastructure’s Gadi high performance computer.\n\nMy role\n\nDeveloped and integrated processes for non-human samples into an existing public workflow\nRealigned and called short variants at NCI Gadi\nGenerated a large-scale multi-breed database of short variants\nOnboarded and supported researcher in accessing and using NCI Gadi HPC\n\n\n\nOutcomes\n\nPublication: GWAS for Chronic Superficial Keratitis in the Australian Racing Greyhound\nPublication: De-novo and genome-wide meta-analyses identify a risk haplotype for congenital sensorineural deafness in Dalmatian dogs"
  },
  {
    "objectID": "projects/wildcats.html",
    "href": "projects/wildcats.html",
    "title": "Cross-species applications of genomic tools",
    "section": "",
    "text": "While genomics has enabled vast improvements in the quantification of genome-wide diversity and the identification of adaptive and deleterious alleles in model species, wildlife species have not reaped the same benefits. In resource constrained species, alternative genomic approaches that reduce costs and computational resources can be used to inform conservation management. In the absence of species-specific reference genomes, the availability of a reference genome from a closely related species can provide a reliable substrate for performing variant discovery. Domestic cats benefit from high-quality reference genome assemblies and share a high degree of genomic synteny with their wild counterparts. In this project, I evaluated and demonstrated the value of cross-species application of various genomic resources developed for the domesitc cat in the Sumatran tiger, snow leopard and cheetah.\n\nMy role\n\nConceptualised all aspects of the research for this project\nLiased with veterinarians and zoo managers to collect samples\nExtracted genomic DNA from whole blood and buccal swabs\nProcessed and analysed whole genome sequencing data\nPerformed in silico simulation of genotyping array and reduced representation sequencing datasets\nPerformed and published a literature review and original manuscript\n\n\n\nOutcomes\n\nPublication: Exploiting genomic synteny in Felidae: cross-species genome alignments and SNV discovery can aid conservation management\nOther results currently under embargo"
  },
  {
    "objectID": "projects/burmese.html",
    "href": "projects/burmese.html",
    "title": "Feline models of complex human disease",
    "section": "",
    "text": "Cat breeds can serve as informative natural genetic models of human disease because of their similarity to humans in terms of genetics, anatomy, physiology, and our shared environment. By studying cat breeds with a high prevalence of a hertiable disease, like feline diabetes mellitus (FDM) in Burmese cats, we can gain insights into the underlying genetic mechanisms of the diseases.\nFor this project, we explored the genetic landscape of FDM in a population of Australian and European Burmese cats. We used multiple techniques and technologies to identify regions of disease association and selection in affected cats.\n\nMy role\n\nConceptualised all aspects of the research for this project\nLiased with veterinarians and breeders to collect samples\nExtracted genomic DNA from whole blood and buccal swabs\nProcessed and analysed Illumina genotyping array and whole genome sequencing data\nIdentified candidate genes and variants for FDM\nPerformed and published a literature review and original manuscript\n\n\n\nOutcomes\n\nWorkflow: Probe Remap workflow\nPublication: The Burmese cat as a genetic model of type 2 diabetes in humans\nPublication: Mapping the genetic basis of diabetes mellitus in the Australian Burmese cat (Felis catus)"
  },
  {
    "objectID": "projects/cancer.html",
    "href": "projects/cancer.html",
    "title": "Precision diagnostics in thyroid cancer",
    "section": "",
    "text": "Whole-genome profiling of somatic mutations allows researchers to develop new insights into the underlying causes of cancers and develop new treatments and prevention strategies. However, genetic profiling of poorly understood cancers can be challenging due to the complexity and heterogeneity of somatic mutations across patient populations, and lacking population-level data. Best practice bioinformatics workflows that provide a standardised approach to somatic mutation identification and annotation can address these challenges and provide clinicians with viable treatment targets.\n\nMy role\n\nIdentified and implemented best practice bioinformatic tools and public datasets\nImplemented 2 best pratice workflows to provide patient reports and identify short and structural variants in a cohort of thyroid cancer patients\nIdentified candidate genes for poorly differentiated thyroid cancer\n\n\n\nOutcomes\n\nWorkflow under development\nResults under embargo"
  },
  {
    "objectID": "projects/parabricks.html",
    "href": "projects/parabricks.html",
    "title": "Evaluating optimised genomics workflows",
    "section": "",
    "text": "Genomics-based research is computationally intensive work and we are generating data faster and at a higher scale than ever before. Accelerated computing using specialised hardware is key to addressing these intensive data loads.\nI worked closely with partners from the National Computational Infrastructure (NCI), who were looking to make scalable bioinformatics data processing more accessible at their facilities. We worked with NCI to evaluate the performance of NVIDIA Parabricks software suite at their Gadi HPC and benchmark it against highly optimised CPU-based workflows. GPU-accelerated tools, like Parabricks, are currently rare in bioinformatics but have the capacity to significantly reduce the time and computational resources required for complex data processing like mapping and variant calling. The outcomes of this work includes:\n\nA public biological validation performance evaluation protocol\nA public report summarising our findings\nThe availability of NVIDIA Parabricks license at NCI Gadi HPC"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Georgie Samaha",
    "section": "",
    "text": "While bioinformatics is an essential part of life science research, the platforms and tools we use to process and analyse our data are not exactly easy to use, no matter how experienced one may be. My involvement in the Australian BioCommons Platforms projects has focused on providing the Australian life science community with:\n\nPublic scalable data processing workflows optimised for national HPCs\nSupport in finding and accessing the compute resources they need\nTraining that builds confidence for beginners who are learning to work on the command-line"
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Georgie Samaha",
    "section": "Selected projects",
    "text": "Selected projects\n\nTraining\nDecoding Bioinformatics: A Primer For Life Scientists | Recording\nUnlocking nf-core: Customising Workflows For Your Research | Materials\nGetting Started With WGS Mapping and Variant Calling | Recording | Materials\n\n\nSoftware\nGermlineStructuralV-nf | A structural variant calling workflow | GitHub\nTemplate-nf | A straightforward Nextflow template | GitHub | Docs\nconfigBuilder-nf | A simple custom config file generator | GitHub"
  },
  {
    "objectID": "archive/byod.html",
    "href": "archive/byod.html",
    "title": "Making bioinformatics workflows accessible",
    "section": "",
    "text": "Working at the command line is challenging for life scientists, many of whom have received no formal computer science training. The Australian BioCommons Bring Your Own Data platforms project is focused on providing the life science community with curated workflows, tools, training and support across Australian command line infrastructures.\n\nMy role\n\nWorkflows theme lead\nDeveloped public, scalable, reproducible data processing workflows\nConceptualised national training events focused on the accessibility of bioinformatics workflows at the command-line\nDelivered and facilitated workshops and webinars as a part of the Australian BioCommons training programme\nLiased with and coordinated activites involving partners at various computational infrastructures, service providers, research communities across the country\nAdvocated for the needs and interests of the life science community to national computational infrastructures and service providers\n\n\n\nOutcomes\n\nInstallation and optimisation of bioinformatics workflows across command line infrastructures at NCI, Pawsey, QRIScloud, and the University of Melbourne Research Computing Services\nEnabled 60 high-impact publications\nSupported 11 research groups\nDeveloped 19 public workflows\n1,600 genomes processed at national HPCs\n1,000 transcriptomes processed at national HPCs\nUpskilled 1,000 life scientists\n\n\n\nSkills\n\nData analysis, software engineering, project management, verbal and written communication\nDisciplines: bioinformatics\nPlatforms: HPC, cloud, RStudio\nTools: bash, R, python, PBS, SLURM, git, GitHub, Nextflow, Singularity, Docker, sHPC"
  },
  {
    "objectID": "archive/parabricks.html",
    "href": "archive/parabricks.html",
    "title": "Evaluating optimised genomics workflows",
    "section": "",
    "text": "As a part of the Australian BioCommons Bring Your Own Data platforms project, we work closely with partners from the National Computational Infrastructure (NCI), who were looking to make scalable bioinformatics data processing more accessible at their facilities. We worked with NCI to evaluate the performance of NVIDIA Parabricks software suite at their Gadi HPC. GPU-accelerated tools, like Parabricks, are currently rare in bioinformatics but have the capacity to significantly reduce the time and computational resources required for complex data processing like mapping and variant calling.\n\nMy role\n\nDesign and implement a technical performance evaluation protocol\nDesign and implement a biological validation protocol\nRun and compare the performance of multiple optimised genomic workflows at NCI Gadi\nReport findings and recommendations to NCI, NVIDIA, and Australian BioCommons regarding the potential impact of the availability of Parabricks at NCI\n\n\n\nOutcomes\n\nWorkflow: GermlineShortV Biovalidation\nReport: Evaluation of Mapping and Germline Variant Calling Pipelines on Australian High-Performance Computing Facilities\nAvailability of NVIDIA Parabricks license at NCI Gadi HPC\n\n\n\nSkills\n\nData analysis, project management, verbal and written communication\nDisciplines: bioinformatics, genomics\nPlatforms: HPC\n\nTools: bash, PBS, git, GitHub"
  },
  {
    "objectID": "research/structuralV.html",
    "href": "research/structuralV.html",
    "title": "Genomic resources for rare peripheral neuropathies",
    "section": "",
    "text": "Like many other rare and heritable conditions, mapping the causative genetic loci of peripheral neuropathies can be challenging due to a lack of available population-level data and standardised methodologies available to the research community. I have worked closely and collaboratively with the Australian motor neuropathies community to develop reproducible and sensitive protocols for identifying clinically relevant genomic events in patient cohorts and families.\n\nMy role\n\nDesign and implement a protocol for filtering variants under expected modes of inheritance\nDesign and implement a public, sensitive, and reproducible workflow for identifying and annotating structural variation\nIdentify candidate causative genomic events\nLiase and collaborate with bioinformaticians, clinicians, and geneticists to optimise a bioinformatics workflow\n\n\n\nOutcomes\n\nWorkflow: GermlineStructuralV-nf\nResults currently under embargo"
  },
  {
    "objectID": "research/110dogs.html",
    "href": "research/110dogs.html",
    "title": "Canine health and population genomics",
    "section": "",
    "text": "This project involved the processing of a large whole genome sequence dataset comprising 113 samples. To process these samples, we developed and applied two highly scalable workflows to realign all samples to a more recent version of the canine reference genome assembly and call short variants (SNVs and indels) across the genome. The scale of the dataset required the use of the National Computational Infrastructure’s Gadi high performance computer.\n\nMy role\n\nDeveloped and integrated processes for non-human samples into an existing public workflow\nRealigned and called short variants at NCI Gadi\nGenerated a large-scale multi-breed database of short variants\nOnboarded and supported researcher in accessing and using NCI Gadi HPC\n\n\n\nOutcomes\n\nPublication: GWAS for Chronic Superficial Keratitis in the Australian Racing Greyhound\nPublication: De-novo and genome-wide meta-analyses identify a risk haplotype for congenital sensorineural deafness in Dalmatian dogs"
  },
  {
    "objectID": "research/wildcats.html",
    "href": "research/wildcats.html",
    "title": "Cross-species applications of genomic tools",
    "section": "",
    "text": "While genomics has enabled vast improvements in the quantification of genome-wide diversity and the identification of adaptive and deleterious alleles in model species, wildlife species have not reaped the same benefits. In resource constrained species, alternative genomic approaches that reduce costs and computational resources can be used to inform conservation management. In the absence of species-specific reference genomes, the availability of a reference genome from a closely related species can provide a reliable substrate for performing variant discovery. Domestic cats benefit from high-quality reference genome assemblies and share a high degree of genomic synteny with their wild counterparts. In this project, I evaluated and demonstrated the value of cross-species application of various genomic resources developed for the domesitc cat in the Sumatran tiger, snow leopard and cheetah.\n\nMy role\n\nConceptualised all aspects of the research for this project\nLiased with veterinarians and zoo managers to collect samples\nExtracted genomic DNA from whole blood and buccal swabs\nProcessed and analysed whole genome sequencing data\nPerformed in silico simulation of genotyping array and reduced representation sequencing datasets\nPerformed and published a literature review and original manuscript\n\n\n\nOutcomes\n\nPublication: Exploiting genomic synteny in Felidae: cross-species genome alignments and SNV discovery can aid conservation management\nOther results currently under embargo"
  },
  {
    "objectID": "research/burmese.html",
    "href": "research/burmese.html",
    "title": "Feline models of complex human disease",
    "section": "",
    "text": "Cat breeds can serve as informative natural genetic models of human disease because of their similarity to humans in terms of genetics, anatomy, physiology, and our shared environment. By studying cat breeds with a high prevalence of a hertiable disease, like feline diabetes mellitus (FDM) in Burmese cats, we can gain insights into the underlying genetic mechanisms of the diseases.\nFor this project, we explored the genetic landscape of FDM in a population of Australian and European Burmese cats. We used multiple techniques and technologies to identify regions of disease association and selection in affected cats.\n\nMy role\n\nConceptualised all aspects of the research for this project\nLiased with veterinarians and breeders to collect samples\nExtracted genomic DNA from whole blood and buccal swabs\nProcessed and analysed Illumina genotyping array and whole genome sequencing data\nIdentified candidate genes and variants for FDM\nPerformed and published a literature review and original manuscript\n\n\n\nOutcomes\n\nWorkflow: Probe Remap workflow\nPublication: The Burmese cat as a genetic model of type 2 diabetes in humans\nPublication: Mapping the genetic basis of diabetes mellitus in the Australian Burmese cat (Felis catus)"
  },
  {
    "objectID": "research/cancer.html",
    "href": "research/cancer.html",
    "title": "Precision diagnostics in thyroid cancer",
    "section": "",
    "text": "Whole-genome profiling of somatic mutations allows researchers to develop new insights into the underlying causes of cancers and develop new treatments and prevention strategies. However, genetic profiling of poorly understood cancers can be challenging due to the complexity and heterogeneity of somatic mutations across patient populations, and lacking population-level data. Best practice bioinformatics workflows that provide a standardised approach to somatic mutation identification and annotation can address these challenges and provide clinicians with viable treatment targets.\n\nMy role\n\nIdentified and implemented best practice bioinformatic tools and public datasets\nImplemented 2 best pratice workflows to provide patient reports and identify short and structural variants in a cohort of thyroid cancer patients\nIdentified candidate genes for poorly differentiated thyroid cancer\n\n\n\nOutcomes\n\nWorkflow under development\nResults under embargo"
  },
  {
    "objectID": "bioinformatics/webinars.html",
    "href": "bioinformatics/webinars.html",
    "title": "Bioinformatics and HPC training",
    "section": "",
    "text": "Bioinformaticians and life scientists are increasingly turning to high performance computing infrastructure and efficient, scalable workflows as their research becomes more data intensive. Accessing and getting the most out of these facilities can be challenging for many of us who have never received formal computational sciences training. This webinar series introduces the high-performance computing facilities and services available to Australian researchers and ways to access and use them to do high impact research.\n\nMy role\n\nConceptualise and deliver webinars to a national audience\n\n\n\nOutcomes\n\nWebinar: Pro tips for scaling bioinformatics workflows for HPC\nWebinar: Where to go when your bioinformatics runs out of compute\nWebinar: High performance bioinformatics: submitting your best NCMAS application\nWebinar: Getting started with mapping and variant calling on the command line\n\n\n\nSkills\n\nVerbal and written communication\nDisciplines: bioinformatics, high performance computing"
  },
  {
    "objectID": "bioinformatics/nextflow.html",
    "href": "bioinformatics/nextflow.html",
    "title": "Flattening the Nextflow learning curve",
    "section": "",
    "text": "Workflow frameworks, like Nextflow, have recently emerged as solutions to the challenges posed by bioinformatics pipelines. Bioinformatics data processing typically comprises many individual steps that accept and output varying file formats, and require the use of multiple coding languages and pieces of software. While Nextflow offers a nice solution for creating reproducible, portable workflows that are easy to run, it comes at the cost of a steep learning curve. Nextflow’s users need to learn yet another Domain-Specific Language (DSL) and a new and foreign programming paradigm in order to get the most out of it.\nAs a part of the Australian BioCommons Bring Your Own Data platforms project, we focused on Nextflow over other workflow frameworks in our workflow development activities, developer support, and training programme due to its growing popularity in the community. The outcomes of this work includes:\n\nMy participation in round 3 of the nf-core mentorship programme\nThe development of institutional configuration files for Australian national HPC infrastructure (Gadi, Setonix, Nimbus)\nThe delivery of a national online workshop focused on customising nf-core workflows in partnership with Seqera Labs\nThe development of a simple Nextflow workflow template\nThe ongoing development of an automated configuration builder tool"
  },
  {
    "objectID": "bioinformatics.html",
    "href": "bioinformatics.html",
    "title": "Georgie Samaha",
    "section": "",
    "text": "While bioinformatics is an essential part of life science research, the platforms and tools we use to process and analyse our data are not exactly easy to use, no matter how experienced one may be. My involvement in the Australian BioCommons Platforms projects has focused on providing the Australian life science community with:\n\nPublic scalable data processing workflows optimised for national HPCs\nSupport in finding and accessing the compute resources they need\nTraining that builds confidence for beginners who are learning to work on the command-line"
  },
  {
    "objectID": "research/parabricks.html",
    "href": "research/parabricks.html",
    "title": "Evaluating optimised genomics workflows",
    "section": "",
    "text": "As a part of the Australian BioCommons Bring Your Own Data platforms project, we work closely with partners from the National Computational Infrastructure (NCI), who were looking to make scalable bioinformatics data processing more accessible at their facilities. We worked with NCI to evaluate the performance of NVIDIA Parabricks software suite at their Gadi HPC. GPU-accelerated tools, like Parabricks, are currently rare in bioinformatics but have the capacity to significantly reduce the time and computational resources required for complex data processing like mapping and variant calling.\n\nMy role\n\nDesign and implement a technical performance evaluation protocol\nDesign and implement a biological validation protocol\nRun and compare the performance of multiple optimised genomic workflows at NCI Gadi\nReport findings and recommendations to NCI, NVIDIA, and Australian BioCommons regarding the potential impact of the availability of Parabricks at NCI\n\n\n\nOutcomes\n\nWorkflow: GermlineShortV Biovalidation\nReport: Evaluation of Mapping and Germline Variant Calling Pipelines on Australian High-Performance Computing Facilities\nAvailability of NVIDIA Parabricks license at NCI Gadi HPC\n\n\n\nSkills\n\nData analysis, project management, verbal and written communication\nDisciplines: bioinformatics, genomics\nPlatforms: HPC\n\nTools: bash, PBS, git, GitHub"
  },
  {
    "objectID": "bioinformatics/hpc.html",
    "href": "bioinformatics/hpc.html",
    "title": "Bioinformatics and HPC training",
    "section": "",
    "text": "Bioinformaticians and life scientists are increasingly turning to high performance computing infrastructure and efficient, scalable workflows as their research becomes more data intensive. Accessing and getting the most out of these facilities can be challenging for many of us who have never received formal computational sciences training. This webinar series I designed and delivered with colleagues across national computational infrastructure providers introduces the high-performance computing facilities and services available to Australian researchers and ways to access and use them to do high impact research.\n\nWebinar: Pro tips for scaling bioinformatics workflows for HPC\nWebinar: Where to go when your bioinformatics runs out of compute\nWebinar: High performance bioinformatics: submitting your best NCMAS application\nWebinar: Getting started with mapping and variant calling on the command line"
  },
  {
    "objectID": "bioinformatics/parabricks.html",
    "href": "bioinformatics/parabricks.html",
    "title": "Evaluating optimised genomics workflows",
    "section": "",
    "text": "Genomics-based research is computationally intensive work and we are generating data faster and at a higher scale than ever before. Accelerated computing using specialised hardware is key to addressing these intensive data loads.\nI worked closely with partners from the National Computational Infrastructure (NCI), who were looking to make scalable bioinformatics data processing more accessible at their facilities. We worked with NCI to evaluate the performance of NVIDIA Parabricks software suite at their Gadi HPC and benchmark it against highly optimised CPU-based workflows. GPU-accelerated tools, like Parabricks, are currently rare in bioinformatics but have the capacity to significantly reduce the time and computational resources required for complex data processing like mapping and variant calling. The outcomes of this work includes:\n\nA public biological validation performance evaluation protocol\nA public report summarising our findings\nThe availability of NVIDIA Parabricks license at NCI Gadi HPC"
  },
  {
    "objectID": "bioinformatics/bioimage.html",
    "href": "bioinformatics/bioimage.html",
    "title": "Public cloud image for bioinformatics",
    "section": "",
    "text": "For most bioinformatics applications, working at the command-line interface is unavoidable and empowering. The CLI gives users the power to work more efficiently, use scalable computing resources, and work more reproducibly. Learning how to effectively use the CLI environment can be particilarly challenging for life scienctists who don’t receive any formal computer science training.\nTo address the challenges of set up and installation, I worked in partnership with Pawsey Supercomputing Research Center and the National Computational Infrastructure to create a purpose built cloud environment to help researchers working on the command-line. It is called the BioImage and is currently available on Pawsey’s Nimbus cloud platform. We are currently working on making it available on other public research cloud and commercial cloud platforms."
  },
  {
    "objectID": "bioinformatics.html#bioinformatics-tools-and-platforms",
    "href": "bioinformatics.html#bioinformatics-tools-and-platforms",
    "title": "Georgie Samaha",
    "section": "",
    "text": "While bioinformatics is an essential part of life science research, the platforms and tools we use to process and analyse our data are not exactly easy to use, no matter how experienced one may be. My involvement in the Australian BioCommons Platforms projects has focused on providing the Australian life science community with:\n\nPublic scalable data processing workflows optimised for national HPCs\nSupport in finding and accessing the compute resources they need\nTraining that builds confidence for beginners who are learning to work on the command-line"
  },
  {
    "objectID": "projects.html#bioinformatics-tools-and-platforms",
    "href": "projects.html#bioinformatics-tools-and-platforms",
    "title": "Georgie Samaha",
    "section": "",
    "text": "While bioinformatics is an essential part of life science research, the platforms and tools we use to process and analyse our data are not exactly easy to use, no matter how experienced one may be. My involvement in the Australian BioCommons Platforms projects has focused on providing the Australian life science community with:\n\nPublic scalable data processing workflows optimised for national HPCs\nSupport in finding and accessing the compute resources they need\nTraining that builds confidence for beginners who are learning to work on the command-line"
  },
  {
    "objectID": "projects/nextflow.html",
    "href": "projects/nextflow.html",
    "title": "Flattening the Nextflow learning curve",
    "section": "",
    "text": "Workflow frameworks, like Nextflow, have recently emerged as solutions to the challenges posed by bioinformatics pipelines. Bioinformatics data processing typically comprises many individual steps that accept and output varying file formats, and require the use of multiple coding languages and pieces of software. While Nextflow offers a nice solution for creating reproducible, portable workflows that are easy to run, it comes at the cost of a steep learning curve. Nextflow’s users need to learn yet another Domain-Specific Language (DSL) and a new and foreign programming paradigm in order to get the most out of it.\nAs a part of the Australian BioCommons Bring Your Own Data platforms project, we focused on Nextflow over other workflow frameworks in our workflow development activities, developer support, and training programme due to its growing popularity in the community. The outcomes of this work includes:\n\nMy participation in round 3 of the nf-core mentorship programme\nThe development of institutional configuration files for Australian national HPC infrastructure (Gadi, Setonix, Nimbus)\nThe delivery of a national online workshop focused on customising nf-core workflows in partnership with Seqera Labs\nThe development of a simple Nextflow workflow template\nThe ongoing development of an automated configuration builder tool"
  },
  {
    "objectID": "projects/bioimage.html",
    "href": "projects/bioimage.html",
    "title": "Public cloud image for bioinformatics",
    "section": "",
    "text": "For most bioinformatics applications, working at the command-line interface is unavoidable and empowering. The CLI gives users the power to work more efficiently, use scalable computing resources, and work more reproducibly. Learning how to effectively use the CLI environment can be particilarly challenging for life scienctists who don’t receive any formal computer science training.\nTo address the challenges of set up and installation, I worked in partnership with Pawsey Supercomputing Research Center and the National Computational Infrastructure to create a purpose built cloud environment to help researchers working on the command-line. It is called the BioImage and is currently available on Pawsey’s Nimbus cloud platform. We are currently working on making it available on other public research cloud and commercial cloud platforms."
  },
  {
    "objectID": "projects/hpc.html",
    "href": "projects/hpc.html",
    "title": "Bioinformatics and HPC training",
    "section": "",
    "text": "Bioinformaticians and life scientists are increasingly turning to high performance computing infrastructure and efficient, scalable workflows as their research becomes more data intensive. Accessing and getting the most out of these facilities can be challenging for many of us who have never received formal computational sciences training. This webinar series I designed and delivered with colleagues across national computational infrastructure providers introduces the high-performance computing facilities and services available to Australian researchers and ways to access and use them to do high impact research.\n\nWebinar: Pro tips for scaling bioinformatics workflows for HPC\nWebinar: Where to go when your bioinformatics runs out of compute\nWebinar: High performance bioinformatics: submitting your best NCMAS application\nWebinar: Getting started with mapping and variant calling on the command line"
  },
  {
    "objectID": "projects.html#selected-projects",
    "href": "projects.html#selected-projects",
    "title": "Georgie Samaha",
    "section": "",
    "text": "While bioinformatics is an essential part of life science research, the platforms and tools we use to process and analyse our data are not exactly easy to use, no matter how experienced one may be. My involvement in the Australian BioCommons Platforms projects has focused on providing the Australian life science community with:\n\nPublic scalable data processing workflows optimised for national HPCs\nSupport in finding and accessing the compute resources they need\nTraining that builds confidence for beginners who are learning to work on the command-line"
  }
]